{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Starter notebook: Fast or Slow with TensorFlow GNN\n","\n","This tutorial is designed to walk competitors of [predict-ai-model-runtime](https://kaggle.com/competitions/predict-ai-model-runtime) through the dataset and using [TensorFlow-GNN](https://github.com/tensorflow/gnn).\n","\n","In summary, you will:\n","\n","- `pip install` libraries\n","- imports helper functions from another project, for reading data (`{layout, tile}_data`) and easier programming of GNN models (`implicit`).\n","- read batches of graphs from the dataset, prints them on screen and explains them.\n","- go through details for writing a GNN model and train it\n","- produce an inference `csv` file on the test set.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:17:25.740429Z","iopub.status.busy":"2023-10-03T15:17:25.740085Z","iopub.status.idle":"2023-10-03T15:18:09.00639Z","shell.execute_reply":"2023-10-03T15:18:09.005309Z","shell.execute_reply.started":"2023-10-03T15:17:25.740403Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow_gnn\n","  Downloading tensorflow_gnn-0.6.0-py3-none-any.whl (803 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.9/803.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting google-vizier>=0.0.13 (from tensorflow_gnn)\n","  Downloading google_vizier-0.1.12-py3-none-any.whl (733 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.0/734.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting ml-collections (from tensorflow_gnn)\n","  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (3.1)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (11.0.0)\n","Requirement already satisfied: apache-beam<2.47.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (2.46.0)\n","Requirement already satisfied: tensorflow>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_gnn) (2.12.0)\n","Requirement already satisfied: protobuf<4,>3.12.2 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.20.3)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.7)\n","Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.9.1)\n","Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam<2.47.0->tensorflow_gnn)\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.2.1)\n","Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.7.4)\n","Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.18)\n","Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.51.1)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.7.0)\n","Requirement already satisfied: httplib2<0.22.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.21.0)\n","Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.23.5)\n","Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.6.1)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (3.13.0)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.22.3)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (1.4.2)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.8.2)\n","Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2023.3)\n","Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2023.6.3)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (4.6.3)\n","Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<2.47.0->tensorflow_gnn) (0.19.0)\n","Collecting pyarrow (from tensorflow_gnn)\n","  Downloading pyarrow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs==23.1.0 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (23.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.4.0)\n","Collecting portpicker>=1.3.1 (from google-vizier>=0.0.13->tensorflow_gnn)\n","  Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n","Collecting grpcio-tools>=1.35.0 (from google-vizier>=0.0.13->tensorflow_gnn)\n","  Downloading grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: googleapis-common-protos>=1.56.4 in /opt/conda/lib/python3.10/site-packages (from google-vizier>=0.0.13->tensorflow_gnn) (1.59.1)\n","Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier>=0.0.13->tensorflow_gnn)\n","  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (68.0.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (2.3.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.10.0->tensorflow_gnn) (0.32.0)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from ml-collections->tensorflow_gnn) (6.0)\n","Collecting contextlib2 (from ml-collections->tensorflow_gnn)\n","  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.10.0->tensorflow_gnn) (0.40.0)\n","INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n","Collecting grpcio-tools>=1.35.0 (from google-vizier>=0.0.13->tensorflow_gnn)\n","  Downloading grpcio_tools-1.59.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.58.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25h  Downloading grpcio_tools-1.57.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.56.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n","  Downloading grpcio_tools-1.56.0rc2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.55.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.54.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.54.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading grpcio_tools-1.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.54.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.53.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.53.0rc2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.52.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.51.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.51.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.50.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.49.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.49.0rc3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.49.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Downloading grpcio_tools-1.48.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<2.47.0->tensorflow_gnn) (0.6.2)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.22.0,>=0.8->apache-beam<2.47.0->tensorflow_gnn) (3.0.9)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.10.0->tensorflow_gnn) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.10.0->tensorflow_gnn) (1.11.2)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from portpicker>=1.3.1->google-vizier>=0.0.13->tensorflow_gnn) (5.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam<2.47.0->tensorflow_gnn) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier>=0.0.13->tensorflow_gnn) (2.0.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (2.20.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (2.3.7)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.10.0->tensorflow_gnn) (3.2.2)\n","Building wheels for collected packages: ml-collections, dill, sqlalchemy\n","  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=da58938d4b778b5e0188a09109228aaf4f11027c1ccd7143f97ef32e26efc670\n","  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n","  Building wheel for dill (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=982dcdf6dbf49d719c5d1a369c66327881d6bb2bce08d6d761642110b0081330\n","  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n","  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1490655 sha256=57a1fb1421876c1cd7abf499c24e5347eb2f485480e1d183d3acd7d920a864f0\n","  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n","Successfully built ml-collections dill sqlalchemy\n","Installing collected packages: sqlalchemy, pyarrow, portpicker, grpcio-tools, dill, contextlib2, ml-collections, google-vizier, tensorflow_gnn\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 2.0.17\n","    Uninstalling SQLAlchemy-2.0.17:\n","      Successfully uninstalled SQLAlchemy-2.0.17\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 11.0.0\n","    Uninstalling pyarrow-11.0.0:\n","      Successfully uninstalled pyarrow-11.0.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.7\n","    Uninstalling dill-0.3.7:\n","      Successfully uninstalled dill-0.3.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","beatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 9.0.0 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n","multiprocess 0.70.15 requires dill>=0.3.7, but you have dill 0.3.1.1 which is incompatible.\n","pathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.1.1 which is incompatible.\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed contextlib2-21.6.0 dill-0.3.1.1 google-vizier-0.1.12 grpcio-tools-1.48.2 ml-collections-0.1.1 portpicker-1.6.0 pyarrow-9.0.0 sqlalchemy-1.4.20 tensorflow_gnn-0.6.0\n","Collecting tensorflow_ranking\n","  Downloading tensorflow_ranking-0.5.3-py2.py3-none-any.whl (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.4.0)\n","Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.23.5)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (1.16.0)\n","Requirement already satisfied: tensorflow-serving-api<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_ranking) (2.12.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.51.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.20.3)\n","Requirement already satisfied: tensorflow<3,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.12.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (68.0.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.11.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.20.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.3.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow_ranking) (3.2.2)\n","Installing collected packages: tensorflow_ranking\n","Successfully installed tensorflow_ranking-0.5.3\n"]}],"source":["!pip install tensorflow_gnn --pre\n","!pip install tensorflow_ranking"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Oct 18 03:20:15 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 530.47                 Driver Version: 531.68       CUDA Version: 12.1     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA GeForce RTX 3090 Ti      On | 00000000:01:00.0  On |                  Off |\n","|  0%   46C    P5               33W / 450W|   1765MiB / 24564MiB |     30%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n","|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n","|    0   N/A  N/A        22      G   /Xwayland                                 N/A      |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:18:09.009357Z","iopub.status.busy":"2023-10-03T15:18:09.008312Z","iopub.status.idle":"2023-10-03T15:18:18.308566Z","shell.execute_reply":"2023-10-03T15:18:18.307467Z","shell.execute_reply.started":"2023-10-03T15:18:09.009326Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-10-18 03:20:22.934643: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-10-18 03:20:23.127417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["# Install standard modules\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","import tensorflow_ranking as tfr"]},{"cell_type":"markdown","metadata":{},"source":["The utility modules are based on the code on [github]():\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:18:18.31043Z","iopub.status.busy":"2023-10-03T15:18:18.309836Z","iopub.status.idle":"2023-10-03T15:18:50.505912Z","shell.execute_reply":"2023-10-03T15:18:50.504499Z","shell.execute_reply.started":"2023-10-03T15:18:18.3104Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tpugraphsv1_implicit_py'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Install utility modules.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# import tpugraphsv1_layout_data_py as layout_data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# import tpugraphsv1_tile_data_py as tile_data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nathanbian/Documents/google-tpu-comp/starter-notebook-fast-or-slow-with-tensorflow-gnn.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtpugraphsv1_implicit_py\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mimplicit\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tpugraphsv1_implicit_py'"]}],"source":["# Install utility modules.\n","\n","# import tpugraphsv1_layout_data_py as layout_data\n","# import tpugraphsv1_tile_data_py as tile_data\n","import tpugraphsv1_implicit_py as implicit"]},{"cell_type":"markdown","metadata":{},"source":["# Training Pipelines\n","\n","The following code is organized as:\n","\n","1.  Helper functions: MLP (`_mlp`) and Embedding layer (`_Opembedding`). The embedding layer amends a feature on the `op` nodes, with name `op_e`, by embedding the integral op IDs.\n","1.  Pipeline code for training on the Layout collections.\n","1.  Pipeline code for training on the Tile collection.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions, for both Layout and Tile collections.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:18:50.508945Z","iopub.status.busy":"2023-10-03T15:18:50.508611Z","iopub.status.idle":"2023-10-03T15:18:50.523111Z","shell.execute_reply":"2023-10-03T15:18:50.52159Z","shell.execute_reply.started":"2023-10-03T15:18:50.508916Z"},"trusted":true},"outputs":[],"source":["def _mlp(dims, hidden_activation, l2reg=1e-4, use_bias=True):\n","    \"\"\"Helper function for multi-layer perceptron (MLP).\"\"\"\n","    layers = []\n","    for i, dim in enumerate(dims):\n","        if i > 0:\n","            layers.append(tf.keras.layers.Activation(hidden_activation))\n","        layers.append(\n","            tf.keras.layers.Dense(\n","                dim,\n","                kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n","                use_bias=use_bias,\n","            )\n","        )\n","    return tf.keras.Sequential(layers)\n","\n","\n","class _OpEmbedding(tf.keras.Model):\n","    \"\"\"Embeds GraphTensor.node_sets['op']['op'] nodes into feature 'op_e'.\"\"\"\n","\n","    def __init__(self, num_ops: int, embed_d: int, l2reg: float = 1e-4):\n","        super().__init__()\n","        self.embedding_layer = tf.keras.layers.Embedding(\n","            num_ops, embed_d, activity_regularizer=tf.keras.regularizers.l2(l2reg)\n","        )\n","\n","    def call(\n","        self, graph: tfgnn.GraphTensor, training: bool = False\n","    ) -> tfgnn.GraphTensor:\n","        op_features = dict(graph.node_sets[\"op\"].features)\n","        op_features[\"op_e\"] = self.embedding_layer(\n","            tf.cast(graph.node_sets[\"op\"][\"op\"], tf.int32)\n","        )\n","        return graph.replace_features(node_sets={\"op\": op_features})"]},{"cell_type":"markdown","metadata":{},"source":["# Layout Training Pipeline\n","\n","We start by defining constants:\n","\n","1. Batch sizes = num graphs, num sampled nodes per graph, and num configurations per graph.\n","1. Collection to train on: source (`xla` versus `nlp`) and search stragey (`random` versus `default`).\n","\n","Then, boilerplate code to prepare the datasets.\n","\n","Then, we dive deeper into the dataset examples (a batch of graphs from the tiles collection).\n","\n","Finally, details on defining a model.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Define constants and choose subcollection\n","\n","We load `BATCH_SIZE` graphs per batch. Each will have\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:18:50.524947Z","iopub.status.busy":"2023-10-03T15:18:50.524544Z","iopub.status.idle":"2023-10-03T15:18:50.538536Z","shell.execute_reply":"2023-10-03T15:18:50.537255Z","shell.execute_reply.started":"2023-10-03T15:18:50.524909Z"},"trusted":true},"outputs":[],"source":["LAYOUT_DATA_ROOT = \"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout\"\n","SOURCE = \"xla\"  # Can be \"xla\" or \"nlp\"\n","SEARCH = \"random\"  # Can be \"random\" or \"default\"\n","\n","# Batch size information.\n","BATCH_SIZE = 16  # Number of graphs per batch.\n","CONFIGS_PER_GRAPH = (\n","    5  # Number of configurations (features and target values) per graph.\n",")\n","MAX_KEEP_NODES = 1000  # Useful for dropout.\n","# `MAX_KEEP_NODES` is (or, is not) useful for Segment Dropout, if model uses\n","# edges \"sampled_config\" and \"sampled_feed\" (or, \"config\" and \"feed\")"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare `tf.data.Dataset` instances\n","\n","Specifically, `layout_train_ds` and `layout_valid_ds`.\n","\n","It can take ~10 minutes if you are running for the first time, for the caches to be created.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:18:50.541076Z","iopub.status.busy":"2023-10-03T15:18:50.540629Z","iopub.status.idle":"2023-10-03T15:20:51.210585Z","shell.execute_reply":"2023-10-03T15:20:51.209234Z","shell.execute_reply.started":"2023-10-03T15:18:50.541036Z"},"trusted":true},"outputs":[],"source":["layout_data_root_dir = os.path.join(\n","    os.path.expanduser(LAYOUT_DATA_ROOT), SOURCE, SEARCH\n",")\n","\n","layout_npz_dataset = layout_data.get_npz_dataset(\n","    layout_data_root_dir,\n","    min_train_configs=CONFIGS_PER_GRAPH,\n","    max_train_configs=500,  # If any graph has more than this configurations, it will be filtered [speeds up loading + training]\n","    cache_dir=\"cache\",\n",")\n","\n","\n","def pair_layout_graph_with_label(graph: tfgnn.GraphTensor):\n","    \"\"\"Extracts label from graph (`tfgnn.GraphTensor`) and returns a pair of `(graph, label)`\"\"\"\n","    # Return runtimes divded over large number: only ranking is required. The\n","    # runtimes are in the 100K range\n","    label = tf.cast(graph.node_sets[\"g\"][\"runtimes\"], tf.float32) / 1e7\n","    return graph, label\n","\n","\n","layout_train_ds = (\n","    layout_npz_dataset.train.get_graph_tensors_dataset(\n","        CONFIGS_PER_GRAPH, max_nodes=MAX_KEEP_NODES\n","    )\n","    .shuffle(100, reshuffle_each_iteration=True)\n","    .batch(BATCH_SIZE, drop_remainder=False)\n","    .map(tfgnn.GraphTensor.merge_batch_to_components)\n","    .map(pair_layout_graph_with_label)\n",")\n","\n","layout_valid_ds = (\n","    layout_npz_dataset.validation.get_graph_tensors_dataset(CONFIGS_PER_GRAPH)\n","    .batch(BATCH_SIZE, drop_remainder=False)\n","    .map(tfgnn.GraphTensor.merge_batch_to_components)\n","    .map(pair_layout_graph_with_label)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Familiarize yourself with data\n","\n","Lets obtain an example from the dataset `layout_train_ds`, i.e., an instance of `GraphTensor` which encodes a batch\n","of graphs. Luckily, using TF-GNN, we can describe our model as-if we are operating on a single graph, and naturally the\n","model extends to multiple graphs!\n","\n","Let's take one example (containing a batch) and print it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:51.212425Z","iopub.status.busy":"2023-10-03T15:20:51.212106Z","iopub.status.idle":"2023-10-03T15:20:54.991191Z","shell.execute_reply":"2023-10-03T15:20:54.990139Z","shell.execute_reply.started":"2023-10-03T15:20:51.212398Z"},"trusted":true},"outputs":[],"source":["graph_batch, config_runtimes = next(iter(layout_train_ds.take(1)))\n","\n","print(\"graph_batch = \")\n","print(graph_batch)\n","print(\"\\n\\n\")\n","print(\"config_runtimes=\")\n","print(config_runtimes)"]},{"cell_type":"markdown","metadata":{},"source":["**< Crash-course on TF-GNN >**\n","\n","Each `GraphTensor` contains three fields:\n","\n","1. `node_sets`, can be thought of `dict` from node type (str) in the graph (batch) to feature tensors for that node type.\n","1. `edge_sets`, can be thought of `dict` from edge type (str) in the graph (batch) to adjacency, as two integer vectors: source IDs and target IDs -- i.e., all edges are directed, unless explicitly undirected by the model. If edge set `e` connects from node-set `n1` to node-set `n2`, then if `graph.edge_sets[\"e1\"].adjacency.source = [0, 13, ...]` and `graph.edge_sets[\"e1\"].adjacency.target = [1, 14, ...]` (must be of equal length), then node `0` from node-set `n1` points to node `1` from node-set `n2`. The IDs are zero-based, and used to index into the feature tensors at `graph.node_sets[\"n1\"]` and `graph.node_sets[\"n2\"]`.\n","1. `context`, contains information per graph in the batch. We do not use this, for the layout collection, as we have singleton nodeset per graph with name `\"g\"` (with features accessible as `graph.node_sets[\"g\"]`)\n","\n","**</ Crash-course on TF-GNN >**\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, lets print the node-sets and the edge-sets of the example `graph_batch`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:54.993028Z","iopub.status.busy":"2023-10-03T15:20:54.992651Z","iopub.status.idle":"2023-10-03T15:20:55.004804Z","shell.execute_reply":"2023-10-03T15:20:55.003673Z","shell.execute_reply.started":"2023-10-03T15:20:54.992983Z"},"trusted":true},"outputs":[],"source":["# The `graph_batch` contains node-sets and edge-sets.\n","# There are no context features for layout collection\n","print(\"graph_batch.context =\", graph_batch.context)\n","# Note: graph_batch.context.sizes must be equal to BATCH_SIZE.\n","# Lets print-out all features for all nodesets.\n","\n","for node_set_name in sorted(graph_batch.node_sets.keys()):\n","    print(f'\\n\\n #####  NODE SET \"{node_set_name}\" #########')\n","    print(\"** Has sizes: \", graph_batch.node_sets[node_set_name].sizes)\n","    for feature_name in graph_batch.node_sets[node_set_name].features.keys():\n","        print(f'\\n Feature \"{feature_name}\" has values')\n","        print(graph_batch.node_sets[node_set_name][feature_name])"]},{"cell_type":"markdown","metadata":{},"source":["**The node set `'g'` corresponds to the \"graph-level\"**. Since `BATCH_SIZE==16`, each tensor in `'g'` should have a leading dimension of `16`. The `graph_id` feature contains model names. Since `CONFIGS_PER_GRAPH_PER_EPOCH=5`, then feature 'runtimes' must be of shape `(16, 5)` with `graph_batch.node_sets['g']['runtimes'][i, j]` indicating the runtime when compiling graph `i` with configuration features `j`. These specific feature values must be found in `nconfig` node-set, as explained next.\n","\n","**Lets look at nodes per graph**. For instance, node set `op` contains the operation nodes in the tensorflow graph (e.g., element-wise add, matrix multiply, etc). Op-codes are stored in `graph_batch.node_sets['op']['op']`. Since each graph has variable number of nodes, the array `graph_batch.node_sets['op'].sizes` gives the number of `op` nodes per (of the `16`) graphs.\n","\n","Some nodes are configurable. The (_virtual_) node-set `nconfig` contains features for configurable nodes. The features are in `graph_batch.node_sets['nconfig']['feats']`.\n","\n","The edge-set `'config'` (next) indicates the correspondence between `nconfig` features and `op` nodes. Specifically, each (_virtual_) `config` node has degree of 1 and each `op` node has degree of 0 or 1 (on edge-set `'config'`).\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's print-out all the edge-sets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.006956Z","iopub.status.busy":"2023-10-03T15:20:55.006545Z","iopub.status.idle":"2023-10-03T15:20:55.022121Z","shell.execute_reply":"2023-10-03T15:20:55.020919Z","shell.execute_reply.started":"2023-10-03T15:20:55.006916Z"},"trusted":true},"outputs":[],"source":["print(\"\\n config edge set: \", graph_batch.edge_sets[\"config\"])\n","print(\"\\n config source nodes: \", graph_batch.edge_sets[\"config\"].adjacency.source)\n","print(\"\\n config target nodes: \", graph_batch.edge_sets[\"config\"].adjacency.target)\n","print(\"\\n g_op edge set: \", graph_batch.edge_sets[\"g_op\"])\n","print(\"\\n g_config edge set: \", graph_batch.edge_sets[\"g_config\"])"]},{"cell_type":"markdown","metadata":{},"source":["The edge-set `'config'` pairs each `\"nconfig\"` node with one `\"op\"` node. To list the correspondences, you print the `.adjacency.source` and `.adjacency.target`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.027021Z","iopub.status.busy":"2023-10-03T15:20:55.026653Z","iopub.status.idle":"2023-10-03T15:20:55.036174Z","shell.execute_reply":"2023-10-03T15:20:55.035155Z","shell.execute_reply.started":"2023-10-03T15:20:55.026991Z"},"trusted":true},"outputs":[],"source":["print(\n","    graph_batch.edge_sets[\"config\"]\n",")  # Holds directed adjacency as list of pairs of indices: nconfig->op\n","print(\n","    graph_batch.edge_sets[\"config\"].adjacency.source\n",")  # Print nconfig indices (should be a range())\n","print(\n","    graph_batch.edge_sets[\"config\"].adjacency.target\n",")  # Print corresponding `op` indices."]},{"cell_type":"markdown","metadata":{},"source":["Other than `config` edges, the remainder of the edge-sets are:\n","\n","```\n","'feed', 'g_op', 'g_config', 'sampled_config', 'sampled_feed'\n","```\n","\n","The first (`feed`) is the actual computation graph! `op` nodes feed into `op` nodes. **Note: The \"transpose\" of this adjacency (implicit) matrix indicates the direction of information flow (models are later in the tutorial).**. The second (`g_op`) and third (`g_config`), group by graph, respectively, `op` nodes and the (virtual) `nconfig` nodes. This edge-set can be helpful for global-pooling operations.\n","\n","_Segment-level Training_: Finally, to implement some version of **dropout**, `sampled_config` and `sampled_feed` edge-sets contain edges to randomly-sampled `op` nodes. To do full-graph (training or inference), you may use `config` and `feed`. To do training with segment dropout (e.g., a naive version of https://arxiv.org/abs/2308.13490, to appear @ NeurIPS'23), you may use `sampled_config` and `sampled_feed`. You may adjust the number of **keep** nodes by setting `MAX_KEEP_NODES`. An edge only survives in `sampled_feed` only if both of its endpoints survived (segment-level) dropout. In our naive implementation here, nodes with contiguous indices are kept. However, you are welcome to re-implement a better segmentation strategy.\n","\n","_NOTE: When using TF-GNN (models to follow), you dont have to worry about `sizes`: just write your model code as-if you are operating on a single graph, and the code naturally extends to a batch of graphs._\n"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling\n","\n","Before we define the full model (`ResModel`), lets run some modeling functions. For example, let's embed the op-codes.\n","\n","We have `layout_npz_dataset.num_ops` unique number of op codes, which determines the embedding size.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.037609Z","iopub.status.busy":"2023-10-03T15:20:55.037291Z","iopub.status.idle":"2023-10-03T15:20:55.113632Z","shell.execute_reply":"2023-10-03T15:20:55.112331Z","shell.execute_reply.started":"2023-10-03T15:20:55.037582Z"},"trusted":true},"outputs":[],"source":["num_ops = layout_npz_dataset.num_ops\n","print(\"number of ops in the dataset=\", num_ops)\n","\n","embedding_layer = _OpEmbedding(\n","    num_ops, 16\n",")  # 16-dimensional embedding, for demonstration.\n","graph_batch_embedded_ops = embedding_layer(graph_batch)\n","\n","print('\\n\\n Before embedding, node-set \"op\"=\\n', graph_batch.node_sets[\"op\"])\n","print(\n","    '\\n\\n After embedding, node-set \"op\"=\\n', graph_batch_embedded_ops.node_sets[\"op\"]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["_Note: after embedding, an additional feature `\"op_e\"` shows-up._\n","\n","Now, lets concatenate the configuration features with the embedding features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.115334Z","iopub.status.busy":"2023-10-03T15:20:55.115027Z","iopub.status.idle":"2023-10-03T15:20:55.122493Z","shell.execute_reply":"2023-10-03T15:20:55.121077Z","shell.execute_reply.started":"2023-10-03T15:20:55.115309Z"},"trusted":true},"outputs":[],"source":["op_e = graph_batch_embedded_ops.node_sets[\"op\"][\"op_e\"]\n","config_features = graph_batch_embedded_ops.node_sets[\"nconfig\"][\"feats\"]\n","\n","print(\"op_e.shape ==\", op_e.shape)\n","print(\"config_features.shape ==\", config_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["There are two differences in the shapes, yet, we concatenate them.\n","\n","1. `op_e` has more nodes: every node has an op-code, but not every node is configurable. We first to resize the leading dimension of `config_features` to equal the leading dimension of `op_e`, by filling zeros for nodes that are not configurable.\n","1. `config_features` is cuboid. The middle dimension identifies the configuration: there are `CONFIGS_PER_GRAPH` of them.\n","\n","For the first, we can multiply by the (sparse) \"config\" adjacency matrix -- a binary matrix where every is a one-hot and most rows are zero. If adjacency entry at `[i, j]` is set, then `graph.node_sets[\"nconfig\"][\"feats\"][j]` contain configuration features for node `i` of `graph.node_sets[\"op\"]`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.124334Z","iopub.status.busy":"2023-10-03T15:20:55.1239Z","iopub.status.idle":"2023-10-03T15:20:55.153467Z","shell.execute_reply":"2023-10-03T15:20:55.152386Z","shell.execute_reply.started":"2023-10-03T15:20:55.124291Z"},"trusted":true},"outputs":[],"source":["config_adj = implicit.AdjacencyMultiplier(graph_batch_embedded_ops, \"config\")\n","print(\"config_adj.shape =\", config_adj.shape)\n","resized_config_features = config_adj @ config_features\n","print(\"resized_config_features.shape =\", resized_config_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Now, we want to broadcast the `op_e` feature matrix to a cuboid, by replicating on a (new) inner dimension so that we can finally combine the config features with op-embeddings.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.155272Z","iopub.status.busy":"2023-10-03T15:20:55.154845Z","iopub.status.idle":"2023-10-03T15:20:55.205184Z","shell.execute_reply":"2023-10-03T15:20:55.204051Z","shell.execute_reply.started":"2023-10-03T15:20:55.15524Z"},"trusted":true},"outputs":[],"source":["broadcasted_op_e = tf.stack([op_e] * CONFIGS_PER_GRAPH, axis=1)\n","\n","combined_features = tf.concat([broadcasted_op_e, resized_config_features], axis=-1)\n","\n","print(\"combined_features.shape = \", combined_features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Now, we want to do graph convolution layer (i.e., message-passing followed by non-linearity) among the `feed` edges. Usually, this can be done by left-multiplying the feature tensor with **some form** of an adjacency matrix. The exact form will determine the pooling (e.g., sum VS average). Let us use the symmetrically-normalized adjacency matrix with self-connections added (by Kipf & Welling, ICLR'17).\n","\n","We can compute such a matrix $\\widehat{A}$ as:\n","\n","$$A_\\textrm{undirected.w.selfconnections} \\leftarrow A + A^\\top + I$$\n","\n","$$D \\leftarrow \\mathbf{1}^\\top A_\\textrm{undirected.w.selfconnections}$$\n","\n","$$\\widehat{A} \\leftarrow D^{-\\frac{1}{2}} (A_\\textrm{undirected.w.selfconnections}) D^{-\\frac{1}{2}} $$\n","\n","Which is acheivable by the following code:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.206654Z","iopub.status.busy":"2023-10-03T15:20:55.206265Z","iopub.status.idle":"2023-10-03T15:20:55.288568Z","shell.execute_reply":"2023-10-03T15:20:55.287552Z","shell.execute_reply.started":"2023-10-03T15:20:55.206615Z"},"trusted":true},"outputs":[],"source":["adj_op_op = implicit.AdjacencyMultiplier(graph_batch_embedded_ops, \"feed\")  # op->op\n","adj_config = implicit.AdjacencyMultiplier(\n","    graph_batch_embedded_ops, \"config\"\n",")  # nconfig->op\n","\n","adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n","adj_op_op_hat = adj_op_op_hat.normalize_symmetric()"]},{"cell_type":"markdown","metadata":{},"source":["Finally, the message passing can written as:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:20:55.290522Z","iopub.status.busy":"2023-10-03T15:20:55.290091Z","iopub.status.idle":"2023-10-03T15:21:00.051943Z","shell.execute_reply":"2023-10-03T15:21:00.050316Z","shell.execute_reply.started":"2023-10-03T15:20:55.290481Z"},"trusted":true},"outputs":[],"source":["A_times_X = adj_op_op_hat @ combined_features\n","print(\"A_times_x.shape =\", A_times_X.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Now, we put together everything above to write a model class `ResModel` (next), which has a couple more concepts:\n","\n","1. Adjacency for `\"g_op\"` and `\"g_config\"`, which is used to pool information from all ops and from configurable ops, to the graph level.\n","1. Residual connections.\n","1. Segment dropout. a forward-pass is computed on the entire graph (but, with `tf.stop_gradient`). Then, another forward pass is computed using only sampled edge-sets (`edgeset_prefix` is set to `\"sampled_\"` by `forward()`).\n","\n","Without further ado, `ResModel`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:21:00.054944Z","iopub.status.busy":"2023-10-03T15:21:00.054444Z","iopub.status.idle":"2023-10-03T15:21:00.075549Z","shell.execute_reply":"2023-10-03T15:21:00.074267Z","shell.execute_reply.started":"2023-10-03T15:21:00.054902Z"},"trusted":true},"outputs":[],"source":["class ResModel(tf.keras.Model):\n","    \"\"\"GNN with residual connections.\"\"\"\n","\n","    def __init__(\n","        self,\n","        num_configs: int,\n","        num_ops: int,\n","        op_embed_dim: int = 32,\n","        num_gnns: int = 2,\n","        mlp_layers: int = 2,\n","        hidden_activation: str = \"leaky_relu\",\n","        hidden_dim: int = 32,\n","        reduction: str = \"sum\",\n","    ):\n","        super().__init__()\n","        self._num_configs = num_configs\n","        self._num_ops = num_ops\n","        self._op_embedding = _OpEmbedding(num_ops, op_embed_dim)\n","        self._prenet = _mlp([hidden_dim] * mlp_layers, hidden_activation)\n","        self._gc_layers = []\n","        for _ in range(num_gnns):\n","            self._gc_layers.append(_mlp([hidden_dim] * mlp_layers, hidden_activation))\n","        self._postnet = _mlp([hidden_dim, 1], hidden_activation, use_bias=False)\n","\n","    def call(self, graph: tfgnn.GraphTensor, training: bool = False):\n","        del training\n","        return self.forward(graph, self._num_configs)\n","\n","    def _node_level_forward(\n","        self,\n","        node_features: tf.Tensor,\n","        config_features: tf.Tensor,\n","        graph: tfgnn.GraphTensor,\n","        num_configs: int,\n","        edgeset_prefix=\"\",\n","    ) -> tf.Tensor:\n","        adj_op_op = implicit.AdjacencyMultiplier(\n","            graph, edgeset_prefix + \"feed\"\n","        )  # op->op\n","        adj_config = implicit.AdjacencyMultiplier(\n","            graph, edgeset_prefix + \"config\"\n","        )  # nconfig->op\n","\n","        adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n","        adj_op_op_hat = adj_op_op_hat.normalize_symmetric()\n","\n","        x = node_features\n","\n","        x = tf.stack([x] * num_configs, axis=1)\n","        config_features = 100 * (adj_config @ config_features)\n","        x = tf.concat([config_features, x], axis=-1)\n","        x = self._prenet(x)\n","        x = tf.nn.leaky_relu(x)\n","\n","        for layer in self._gc_layers:\n","            y = x\n","            y = tf.concat([config_features, y], axis=-1)\n","            y = tf.nn.leaky_relu(layer(adj_op_op_hat @ y))\n","            x += y\n","        return x\n","\n","    def forward(\n","        self, graph: tfgnn.GraphTensor, num_configs: int, backprop=True\n","    ) -> tf.Tensor:\n","        graph = self._op_embedding(graph)\n","\n","        config_features = graph.node_sets[\"nconfig\"][\"feats\"]\n","        node_features = tf.concat(\n","            [graph.node_sets[\"op\"][\"feats\"], graph.node_sets[\"op\"][\"op_e\"]], axis=-1\n","        )\n","\n","        x_full = self._node_level_forward(\n","            node_features=tf.stop_gradient(node_features),\n","            config_features=tf.stop_gradient(config_features),\n","            graph=graph,\n","            num_configs=num_configs,\n","        )\n","\n","        if backprop:\n","            x_backprop = self._node_level_forward(\n","                node_features=node_features,\n","                config_features=config_features,\n","                graph=graph,\n","                num_configs=num_configs,\n","                edgeset_prefix=\"sampled_\",\n","            )\n","\n","            is_selected = graph.node_sets[\"op\"][\"selected\"]\n","            # Need to expand twice as `is_selected` is a vector (num_nodes) but\n","            # x_{backprop, full} are 3D tensors (num_nodes, num_configs, num_feats).\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            is_selected = tf.expand_dims(is_selected, axis=-1)\n","            x = tf.where(is_selected, x_backprop, x_full)\n","        else:\n","            x = x_full\n","\n","        adj_config = implicit.AdjacencyMultiplier(graph, \"config\")\n","\n","        # Features for configurable nodes.\n","        config_feats = adj_config.transpose() @ x\n","\n","        # Global pooling\n","        adj_pool_op_sum = implicit.AdjacencyMultiplier(graph, \"g_op\").transpose()\n","        adj_pool_op_mean = adj_pool_op_sum.normalize_right()\n","        adj_pool_config_sum = implicit.AdjacencyMultiplier(\n","            graph, \"g_config\"\n","        ).transpose()\n","        x = self._postnet(\n","            tf.concat(\n","                [\n","                    # (A D^-1) @ Features\n","                    adj_pool_op_mean @ x,\n","                    # l2_normalize( A @ Features )\n","                    tf.nn.l2_normalize(adj_pool_op_sum @ x, axis=-1),\n","                    # l2_normalize( A @ Features )\n","                    tf.nn.l2_normalize(adj_pool_config_sum @ config_feats, axis=-1),\n","                ],\n","                axis=-1,\n","            )\n","        )\n","\n","        x = tf.squeeze(x, -1)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Training loop\n","\n","Create a model, objective function, and optimizer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:21:00.077361Z","iopub.status.busy":"2023-10-03T15:21:00.077024Z","iopub.status.idle":"2023-10-03T15:21:00.153661Z","shell.execute_reply":"2023-10-03T15:21:00.152391Z","shell.execute_reply.started":"2023-10-03T15:21:00.077332Z"},"trusted":true},"outputs":[],"source":["model = ResModel(CONFIGS_PER_GRAPH, layout_npz_dataset.num_ops)\n","\n","loss = tfr.keras.losses.ListMLELoss()  # (temperature=10)\n","opt = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=0.5)\n","\n","model.compile(\n","    loss=loss,\n","    optimizer=opt,\n","    metrics=[\n","        tfr.keras.metrics.OPAMetric(name=\"opa_metric\"),\n","    ],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Train for a few epochs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:21:00.155936Z","iopub.status.busy":"2023-10-03T15:21:00.155331Z","iopub.status.idle":"2023-10-03T15:26:10.32032Z","shell.execute_reply":"2023-10-03T15:26:10.319071Z","shell.execute_reply.started":"2023-10-03T15:21:00.15574Z"},"trusted":true},"outputs":[],"source":["early_stop = (\n","    5  # If validation OPA did not increase in this many epochs, terminate training.\n",")\n","best_params = None  # Stores parameters corresponding to best validation OPA, to restore to them after training.\n","best_val_opa = -1  # Tracks best validation OPA\n","best_val_at_epoch = -1  # At which epoch.\n","epochs = 1  # Total number of training epochs.\n","\n","for i in range(epochs):\n","    history = model.fit(\n","        layout_train_ds,\n","        epochs=1,\n","        verbose=1,\n","        validation_data=layout_valid_ds,\n","        validation_freq=1,\n","    )\n","\n","    train_loss = history.history[\"loss\"][-1]\n","    train_opa = history.history[\"opa_metric\"][-1]\n","    val_loss = history.history[\"val_loss\"][-1]\n","    val_opa = history.history[\"val_opa_metric\"][-1]\n","    if val_opa > best_val_opa:\n","        best_val_opa = val_opa\n","        best_val_at_epoch = i\n","        best_params = {v.ref: v + 0 for v in model.trainable_variables}\n","        print(\" * [@%i] Validation (NEW BEST): %s\" % (i, str(val_opa)))\n","    elif early_stop > 0 and i - best_val_at_epoch >= early_stop:\n","        print(\n","            \"[@%i] Best accuracy was attained at epoch %i. Stopping.\"\n","            % (i, best_val_at_epoch)\n","        )\n","        break\n","\n","# Restore best parameters.\n","print(\"Restoring parameters corresponding to the best validation OPA.\")\n","assert best_params is not None\n","for v in model.trainable_variables:\n","    v.assign(best_params[v.ref])"]},{"cell_type":"markdown","metadata":{},"source":["## Make Submission CSV file for this task\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T15:26:10.322337Z","iopub.status.busy":"2023-10-03T15:26:10.32154Z","iopub.status.idle":"2023-10-03T16:37:30.828518Z","shell.execute_reply":"2023-10-03T16:37:30.825204Z","shell.execute_reply.started":"2023-10-03T15:26:10.322306Z"},"trusted":true},"outputs":[],"source":["import tqdm\n","\n","_INFERENCE_CONFIGS_BATCH_SIZE = 50\n","\n","output_csv_filename = f\"inference_layout_{SOURCE}_{SEARCH}.csv\"\n","print(\"\\n\\n   Running inference on test set ...\\n\\n\")\n","test_rankings = []\n","\n","assert layout_npz_dataset.test.graph_id is not None\n","for graph in tqdm.tqdm(\n","    layout_npz_dataset.test.iter_graph_tensors(),\n","    total=layout_npz_dataset.test.graph_id.shape[-1],\n","    desc=\"Inference\",\n","):\n","    num_configs = graph.node_sets[\"g\"][\"runtimes\"].shape[-1]\n","    all_scores = []\n","    for i in tqdm.tqdm(range(0, num_configs, _INFERENCE_CONFIGS_BATCH_SIZE)):\n","        end_i = min(i + _INFERENCE_CONFIGS_BATCH_SIZE, num_configs)\n","        # Take a cut of the configs.\n","        node_set_g = graph.node_sets[\"g\"]\n","        subconfigs_graph = tfgnn.GraphTensor.from_pieces(\n","            edge_sets=graph.edge_sets,\n","            node_sets={\n","                \"op\": graph.node_sets[\"op\"],\n","                \"nconfig\": tfgnn.NodeSet.from_fields(\n","                    sizes=graph.node_sets[\"nconfig\"].sizes,\n","                    features={\n","                        \"feats\": graph.node_sets[\"nconfig\"][\"feats\"][:, i:end_i],\n","                    },\n","                ),\n","                \"g\": tfgnn.NodeSet.from_fields(\n","                    sizes=tf.constant([1]),\n","                    features={\n","                        \"graph_id\": node_set_g[\"graph_id\"],\n","                        \"runtimes\": node_set_g[\"runtimes\"][:, i:end_i],\n","                        \"kept_node_ratio\": node_set_g[\"kept_node_ratio\"],\n","                    },\n","                ),\n","            },\n","        )\n","        h = model.forward(subconfigs_graph, num_configs=(end_i - i), backprop=False)\n","        all_scores.append(h[0])\n","    all_scores = tf.concat(all_scores, axis=0)\n","    graph_id = graph.node_sets[\"g\"][\"graph_id\"][0].numpy().decode()\n","    sorted_indices = (\n","        tf.strings.join(tf.strings.as_string(tf.argsort(all_scores)), \";\")\n","        .numpy()\n","        .decode()\n","    )\n","    test_rankings.append((graph_id, sorted_indices))\n","\n","with tf.io.gfile.GFile(output_csv_filename, \"w\") as fout:\n","    fout.write(\"ID,TopConfigs\\n\")\n","    for graph_id, ranks in test_rankings:\n","        fout.write(f\"layout:{SOURCE}:{SEARCH}:{graph_id},{ranks}\\n\")\n","print(\"\\n\\n   ***  Wrote\", output_csv_filename, \"\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Combine submission CSVs from all layout collections into one CSV file\n","\n","Finally, after running on all collections, you need to combine the CSV files together (e.g., by concatenation), to prepare the final submission. Specifically, you can modify the constants:\n","\n","```\n","SOURCE = 'xla'  # Can be \"xla\" or \"nlp\"\n","SEARCH = 'random'  # Can be \"random\" or \"default\"\n","```\n","\n","(from a few cells ago) and run for all 4 combinations: SOURCE=(\"xla\", \"nlp\") x SEARCH=(\"random\", \"default\"), then combine all inferences into one file:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-03T16:45:07.412563Z","iopub.status.busy":"2023-10-03T16:45:07.411316Z","iopub.status.idle":"2023-10-03T16:45:08.727883Z","shell.execute_reply":"2023-10-03T16:45:08.726289Z","shell.execute_reply.started":"2023-10-03T16:45:07.412514Z"},"trusted":true},"outputs":[],"source":["!cat inference_layout_xla_random.csv inference_layout_xla_default.csv inference_layout_nlp_random.csv inference_layout_nlp_default.csv > inference_layout_all.csv"]},{"cell_type":"markdown","metadata":{},"source":["producing file `\"inference_layout_all.csv\"` that combines all predictions for all layout subcollections. Finally, this file should be combined with the CSV for the tiles collection, as explained next.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Tile Training Pipeline\n","\n","This section will be written by end of September. We prioritized getting this notebook out, as soon as possible, as the above Layout section is (1) more tricky and (2) most of the score depends on it.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
